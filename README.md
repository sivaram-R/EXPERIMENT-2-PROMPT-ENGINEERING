# EXP-2: Prompt Engineering
## Aim

To perform a comparative analysis of different prompting patterns and evaluate how large language models respond to broad/unstructured prompts versus refined/clear prompts across multiple scenarios.

## Algorithm

1. Select prompt patterns:

  Broad / Unstructured Prompt

  Refined / Structured Prompt

2. Define test scenarios: Summarization, Problem-Solving, Creative Writing, Coding.

3. Provide inputs to the model using both prompt types.

4. Collect and record outputs.

5. Evaluate based on accuracy, depth, and relevance.

6. Compare and conclude findings.

## Experiment Scenarios
### Scenario 1: Summarization

Unstructured Prompt: Tell me about AI.

Refined Prompt: Summarize Artificial Intelligence in under 100 words focusing on its applications in healthcare.

### Scenario 2: Problem-Solving

Unstructured Prompt: Solve this math problem.

Refined Prompt: Solve 25x + 10 = 85 and show step-by-step calculation.

### Scenario 3: Creative Writing

Unstructured Prompt: Write a story.

Refined Prompt: Write a 100-word short story about a robot who learns empathy.

### Scenario 4: Coding Task

Unstructured Prompt: Write code for sorting.

Refined Prompt: Write a Python program to sort a list of integers in ascending order using Bubble Sort algorithm.

## Sample Output Comparison
<img width="873" height="371" alt="image" src="https://github.com/user-attachments/assets/b51416f4-1b16-4365-b4e5-2c4b6df28bd4" />


## Result

Refined prompting consistently produced better, accurate, and more structured outputs than unstructured prompts.
This shows that Prompt Engineering is crucial for optimizing responses from LLMs.
